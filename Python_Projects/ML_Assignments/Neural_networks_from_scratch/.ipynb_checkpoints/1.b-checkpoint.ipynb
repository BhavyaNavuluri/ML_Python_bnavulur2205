{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import csv \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  loading the data set \n",
    "file = pd.read_csv('BSOM_DataSet_for_HW3.csv')\n",
    "\n",
    "\n",
    "# to check whether there are any empty values or not \n",
    "df=file[['all_mcqs_avg_n20', 'all_NBME_avg_n4', 'CBSE_01',  'CBSE_02', 'LEVEL']]\n",
    "\n",
    "\n",
    "df['LEVEL'].fillna(value='A', inplace=True) \n",
    "\n",
    "df[['CBSE_02']].fillna(method ='bfill')\n",
    "#.fillna(52, inplace=True)\n",
    "df.fillna(df['CBSE_02'].mean(), inplace=True)\n",
    "\n",
    "df.isnull().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL = {'A': 1,'B': 2,'C':3,'D':4}\n",
    "\n",
    "df.LEVEL = [LEVEL[item] for item in df.LEVEL] \n",
    "print(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onhtencder = OneHotEncoder(categorical_features = [0]) \n",
    "data = onhtencder.fit_transform(df[['LEVEL']]).toarray() \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df.iloc[:,:]\n",
    "\n",
    "target=df.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,data, test_size=0.20)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sigmoid function to normalize inputs\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# sigmoid derivatives to adjust synaptic weights\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# input dataset\n",
    "\n",
    "training_inputs=np.array(X_train.iloc[:,:-1])\n",
    "training_outputs=np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# seed random numbers to make calculation\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0 to create weight matrix, synaptic weights\n",
    "synaptic_weights = 2 * np.random.random((4,1))\n",
    "\n",
    "print('Random starting synaptc weights: ')\n",
    "print(synaptic_weights)\n",
    "\n",
    "# Iterate 10,000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.io import loadmat\n",
    "#weights = to_matrix('training_inputs')\n",
    "theta1 = 2 * np.random.random((5,5))- 1    #Theta1 has size 25 x 401\n",
    "theta2 = 2 * np.random.random((4,6))- 1    #Theta2 has size 10 x 26\n",
    "nn_params = np.hstack((theta1.ravel(order='F'), theta2.ravel(order='F')))    #unroll parameters\n",
    "# neural network hyperparameters\n",
    "input_layer_size = 4\n",
    "hidden_layer_size = 5\n",
    "num_labels = 4\n",
    "lmbda = 1\n",
    "print('theta1',theta1)\n",
    "print('theta2',theta2)\n",
    "print('nn_params',nn_params)\n",
    "print(nn_params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('nn_params',nn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunc(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):\n",
    "    theta1 = np.reshape(nn_params[:hidden_layer_size*(input_layer_size+1)], (hidden_layer_size, input_layer_size+1), 'F')\n",
    "    theta2 = np.reshape(nn_params[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1), 'F')\n",
    "\n",
    "   \n",
    "    m = len(training_outputs)\n",
    "    #print('m',m)\n",
    "    ones = np.ones((m,1))\n",
    "    #print('ones',ones)\n",
    "    a1 = np.hstack((ones, training_inputs))\n",
    "    #print('a1',a1)\n",
    "    a2 = sigmoid(a1 @ theta1.T)\n",
    "    #print('a2',a2)\n",
    "    a2 = np.hstack((ones, a2))\n",
    "    #print('a2',a2)\n",
    "    h = sigmoid(a2 @ theta2.T)\n",
    "    #print('h',h)\n",
    "\n",
    "    #y_d = pd.get_dummies(y_d.flatten())\n",
    "    #print('y_d',y_d)\n",
    "    \n",
    "\n",
    "    \n",
    "    temp1 = np.multiply(y, np.log(h))\n",
    "    #print('temp1',temp1)\n",
    "    temp2 = np.multiply(1-y, np.log(1-h))\n",
    "    #print('temp2',temp2)\n",
    "    temp3 = np.sum(temp1 + temp2)\n",
    "    #print('temp3',temp3)\n",
    "\n",
    "    sum1 = np.sum(np.sum(np.power(theta1[:,1:],2), axis = 1))\n",
    "    #print('sum1',sum1)\n",
    "    sum2 = np.sum(np.sum(np.power(theta2[:,1:],2), axis = 1))\n",
    "    #print('sum2',sum2)\n",
    "    return np.sum(temp3 / (-m)) + (sum1 + sum2) * lmbda / (2*m)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_er = nnCostFunc(nn_params, input_layer_size, hidden_layer_size, num_labels, training_inputs,training_outputs, lmbda)\n",
    "cf_er.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGrad(z):\n",
    "    return np.multiply(sigmoid(z), 1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(h, y_train):\n",
    "    loss=np.sum((cf_er-y_train)**2)/2.0\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def randInitializeWeights(L_in, L_out):\n",
    "    epsilon = 0.12\n",
    "    return np.random.rand(L_out, L_in+1) * 2 * epsilon - epsilon\n",
    "\n",
    "initial_theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "# unrolling parameters into a single column vector\n",
    "nn_initial_params = np.hstack((initial_theta1.ravel(order='F'), initial_theta2.ravel(order='F')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnGrad(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):\n",
    "    \n",
    "    initial_theta1 = np.reshape(nn_params[:hidden_layer_size*(input_layer_size+1)], (hidden_layer_size, input_layer_size+1), 'F')\n",
    "    initial_theta2 = np.reshape(nn_params[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1), 'F')\n",
    "    \n",
    "    delta1 = np.zeros(initial_theta1.shape)\n",
    "    #print('delta1',delta1)\n",
    "    #print('delta1 shape',delta1.shape)\n",
    "    delta2 = np.zeros(initial_theta2.shape)\n",
    "    #print('delta2',delta2)\n",
    "    #print('delta2 shape',delta2.shape)\n",
    "    m = len(y)\n",
    "\n",
    "    ones = np.ones((m,1))\n",
    "    \n",
    "    \n",
    "    a1 = np.hstack((ones, X))\n",
    "    #print('a1',a1)\n",
    "    #print('al shape',a1.shape)\n",
    "    z2 = a1 @ initial_theta1.T\n",
    "    #print('z2',z2)\n",
    "    #print('z2 sha',z2.shape)\n",
    "    a2 = np.hstack((ones, sigmoid(z2)))\n",
    "    #print('a2 ',a2)\n",
    "    #print('a2 ',a2.shape)\n",
    "    z3 = a2 @ initial_theta2.T\n",
    "    #print('z3',z3)\n",
    "    #print('z3 ',z3.shape)\n",
    "    a3 = sigmoid(z3)\n",
    "    #print('a3',a3)\n",
    "    #print('a3',a3.shape)\n",
    "    y1=pd.DataFrame(y)\n",
    "\n",
    "    d3 = a3 - y\n",
    "    #print('d3',d3)\n",
    "    #print('d3',d3.shape)\n",
    "    z2 = np.hstack((ones, z2))\n",
    "    #print('z2',z2)\n",
    "    #print('z2',z2.shape)\n",
    "    d2 = np.multiply(initial_theta2.T @ d3.T, sigmoidGrad(z2).T)\n",
    "    #print('d2',d2)\n",
    "    #print('d2',d2.shape)\n",
    "    delta1 = delta1 + d2[1:,:] @ a1#[np.newaxis,:]\n",
    "    #print('delta1',delta1)\n",
    "    #print('delta1',delta1.shape)\n",
    "    delta2 = delta2 + d3.T @ a2#[np.newaxis,:]\n",
    "    #print('delta2',delta2)\n",
    "    #print('delta2 ',delta2.shape)\n",
    "        \n",
    "        \n",
    "    delta1 /= m\n",
    "    delta2 /= m\n",
    "    #print(delta1.shape, delta2.shape)\n",
    "    delta1[:,1:] = delta1[:,1:] + initial_theta1[:,1:] * lmbda / m\n",
    "    delta2[:,1:] = delta2[:,1:] + initial_theta2[:,1:] * lmbda / m\n",
    "        \n",
    "    return np.hstack((delta1.ravel(order='F'), delta2.ravel(order='F')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_backprop_Params = nnGrad(nn_initial_params, input_layer_size, hidden_layer_size, num_labels,training_inputs,training_outputs, lmbda)\n",
    "\n",
    "nn_backprop_Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x, theta):\n",
    "    dtheta = x\n",
    "    \n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def checkGradient(nn_initial_params,nn_backprop_Params,input_layer_size, hidden_layer_size, num_labels,myX,myy,mylambda=0.):\n",
    "    myeps = 0.0001\n",
    "    flattened = nn_initial_params\n",
    "    #print('flattened',flattened)\n",
    "    flattenedDs = nn_backprop_Params\n",
    "    #print('flattenedDs',flattenedDs)\n",
    "    n_elems = len(flattened) \n",
    "    \n",
    "    for i in range(10):\n",
    "        x = int(np.random.rand()*n_elems)\n",
    "        #print('x',x)\n",
    "        epsvec = np.zeros((n_elems,1))\n",
    "        #print('epsvec',epsvec)\n",
    "        epsvec[x] = myeps\n",
    "        #print('epsvec[x]',epsvec[x])\n",
    "\n",
    "        cost_high = nnCostFunc(flattened + epsvec.flatten(),input_layer_size, hidden_layer_size, num_labels,myX,myy,mylambda)\n",
    "        #print('cost_high',cost_high)\n",
    "        cost_low  = nnCostFunc(flattened - epsvec.flatten(),input_layer_size, hidden_layer_size, num_labels,myX,myy,mylambda)\n",
    "        #print('cost_low',cost_low)\n",
    "        mygrad = (cost_high - cost_low) / float(2*myeps)\n",
    "        print('mygrad',mygrad)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print('flattenedDs[x]',flattenedDs[x])\n",
    "        #print(\"Element: {0}. Numerical Gradient = {1:.9f}. BackProp Gradient = {2:.9f}.\",x,mygrad,flattenedDs[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad=checkGradient(nn_initial_params,nn_backprop_Params,input_layer_size, hidden_layer_size, num_labels,training_inputs,training_outputs, lmbda)\n",
    "\n",
    "\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "theta_opt = opt.fmin_cg(maxiter = 50, f = nnCostFunc, x0 = nn_initial_params, fprime = nnGrad, \\\n",
    "                        args = (input_layer_size, hidden_layer_size, num_labels,training_inputs,training_outputs, lmbda))\n",
    "\n",
    "theta1_opt = np.reshape(theta_opt[:hidden_layer_size*(input_layer_size+1)], (hidden_layer_size, input_layer_size+1), 'F')\n",
    "theta2_opt = np.reshape(theta_opt[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1), 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1_opt,theta2_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta1, theta2, X, y):\n",
    "    m = len(y)\n",
    "    ones = np.ones((m,1))\n",
    "    a1 = np.hstack((ones, X))\n",
    "    a2 = sigmoid(a1 @ theta1.T)\n",
    "    a2 = np.hstack((ones, a2))\n",
    "    h = sigmoid(a2 @ theta2.T)\n",
    "    return np.argmax(h, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(theta1_opt, theta2_opt, X_test.iloc[:,:-1], y_test)\n",
    "pred\n",
    "#np.mean(pred == y.flatten()) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual=X_test.iloc[:,-1]\n",
    "\n",
    "rmse=np.sqrt( np.sum((pred-y_actual)**2) /len(y_actual))\n",
    "print(rmse)\n",
    "mse=np.sum((pred-y_actual)**2) /len(y_actual)\n",
    "mse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
